[{"id":0,"href":"/cs7210/lectures/lecture03/","title":"03 Time in Distributed Systems","parent":"Lectures","content":"Introduction     Distributed Systems cannot rely on Physical Clocks so they rely on Logical Clocks  Why Do We Need Time     There are many scenarios in which knowing the time can be helpful. For eg. checking the correctness of the program. Same operations executed in different  Determining the order of operations executing in the system Checking correctness of the final state with the expected final state    Why Is Measuring Time Hard in DS?     Option 1: Why can\u0026rsquo;t we use local clock on each node to record the time  Good: Since each message is timestamped, there is a global order of all the events Bad: It\u0026rsquo;s impossible to precisely synchronise all the local clocks   Option 2: Let there be a receiver/observer node. Every node in the system sends it a message when an event occurs on that node.  Due to unpredictable delays, message can arrive at the observer out of order Different Observers can report different order of events    Logical Time     Logical clocks, unlike physical clocks, progress only when an event occurs and usually provides a distinct timestamp to each event. Types of clocks (details left out in lecture)  Scalar Clock / Lamport Clocks Vector Clocks Matrix Clocks    Common Notation     There are three type of events : send of a msg, recv of a msg and internal events in a node Note that when a msg is send from node n1 to node n2, two events occur : n1 registers send(msg) event while n2 registers recv(msg) event. e1 → e2 implies, e1 \u0026ldquo;happened before\u0026rdquo; e2 All events on a single node are ordered If a node receives a msg (e1) and sends it (e2) , then e1 → e2 If a node sends a msg (e1) and it is recvd by another node(e2), then e1 → e2  Concurrent Events     If e1 and e2 are not related by \u0026ldquo;happened before\u0026rdquo; relationship, we call them concurrent. Is means that we cannot conclusively state the order in which those events occurred. e1 || e2  Logical Clocks     Timestamps generated by logical clocks need to guarantee some properties to be useful Clock Consistency Condition  e1 → e2 then, C(e1) \u0026lt; C(e2) Monotonicity property, logical clocks must always increase Note that C(e1) \u0026lt; C(e2) does not imply e1 → e2   e1 || e2 then, C(e1) ?? C(e2)  For concurrent events, there are no guarantees, the logical timestamps can be in any order or they may simply be uncomparable   Strong Clock Consistency (not mandatory for all logical clocks)  e1 → e2 ⇔ C(e1) \u0026lt; C(e2)   A logical clock defines a set of rules on how to advance it and how to compare two timestamps.  Lamport\u0026rsquo;s Scalar Clock     Each process had it\u0026rsquo;s own local logical clock Rules  Vector Clocks (vt)      Lamport Clock does not satisfy the Strong Clock Consistency\n  Size of clock is equal to number of processes in the system\n  If vt is the vector clock at a process, then vt[i] represents that process\u0026rsquo;s view of time of ith process.\n  Rules   Comparison Rules\n  Matrix Clocks     Each process maintain N x N matrix Consider process Pi,  i-th row of matrix corresponds to Pi\u0026rsquo;s own vector clock j-th row (other than i) corresponds to Pi\u0026rsquo;s view of Pj \u0026rsquo;s vector clock   Benefits  A process can know if the vector clock of every other process has progressed past a certain time, t. This can be used to delete any data which is cached for processed which are falling behind.    "},{"id":1,"href":"/cs7210/lectures/lecture04/","title":"04 State in Distributed Systems","parent":"Lectures","content":"Introduction     Study challenges in maintaining state in a distributed system. Study capturing of snapshot of the global state of the Distributed System. This helps to understand what\u0026rsquo;s happening in the system and to calculate its properties Chandy - Lamport Algorithm  Global State, Snapshots and Other Terminology     Distributed system consists of a set of processes and communication channels between them. A Process has a series of events occurring. These events can be of three types:    send(msg)2. recv(msg)3. internal event    Process State is defined by history of events on that process Channel State is defined by inflight message Global State ⇒ (all Process States + all Channel States) Process History, h: [-∞, .., e] Cut : h1 ⋃ h2 ⋃ \u0026hellip; ⋃ hn Actual and Observed Run can be different Run: Ordered sequence of events of a Cut Consistent Cut Snapshot Pre-Recording Event: Events occured before the time of snapshot. Post-Recording Event: Events occurred after the snapshot is recorded.  Challenges About State in Distributed Systems     Instantaneous recording not possible  No Global Clock: As a result, processes cannot capture individual states at exact same time. Unreliable Network: As a result, a node cannot invoke every other node to record state instantaneously   Deterministic vs Non-Deterministic Computation  Distributed computation is non-deterministic unlike a single threaded stand alone program.    System Model     Processes Channels:  Directed, process-to-process FIFO Error free (Messages are not corrupted)   While normal channels are not FIFO and error free. It\u0026rsquo;s possible to build one. For eg. TCP protocol.  Finding a Consistent Cut: Algorithm in Action     Goal: Capture a consistent global state (process states + channel states) State of channel: Consider a scenario where we have a global clock. We are evaluating a system of two process : p and q. p sends msg to q at time t1. q receives msg from p at t3. We take the snapshot at time t2 such that t3 \u0026gt; t2 \u0026gt; t1. The state of channel from p to q will contain the inflight msg.  Snapshot Algorithm     Initiator  Save it\u0026rsquo;s local state Send marker tokens on all outgoing edges   All Other Processes: On receiving the first marker on any incoming edge  Save state, and propagate markers on all outgoing edges Resume Execution but also save incoming messages until a marker arrives through the channel    handleMarker (channel c, Marker m) { if local_state not recorded: record local_state state(c) = EMPTY propage m else state(c) = msg received on c since local_state was recorded } Guarantees a consistent global state!\n Assumptions of the algorithm  Communication channel is FIFO There are no failures and all messages arrive intact and only once The snapshot algo doesn\u0026rsquo;t interfere with the normal execution of the processes Each process in the system records its local state and the state of its incoming channels    Global State    Features of Chandy and Lamport Algorithm:\n Does not promise to give us exactly what is there but it gives us a consistent state (Not Real-time snapshot) The recorded Global State may not correspond to a state that ever really happened at some global time The state, however, does represent a possible global state (a vertex in state lattice diagram from the paper) The observed state respects all the partial order of events imposed by causality.  Properties of the Global State    Benefits of Global State: Evaluate Stable Properties     What good are \u0026ldquo;recorded states\u0026rdquo; if they never really occur? They can still be useful for measuring stable properties. A property is called a stable property iff once it becomes true in a state S, it remains true for all states S’ reachable from S. Note if stable property is false, we can\u0026rsquo;t say anything about it in future states. For example:  Deadlock, If computation is complete in recorded state, then it\u0026rsquo;s also complete in actual terminal state    Definite vs Possible State     Certain unstable properties may also be important. For an unstable prop, there is no guarantee that once it becomes true it remains true forever. eg. buffer overflow, race condition. Is evaluating unstable properties on \u0026ldquo;recorded state\u0026rdquo; useful? Yes. If a undesirable unstable property was true in \u0026ldquo;recorded state\u0026rdquo;, then we know that that property can be true under some conditions in the system. This is useful. We now need to either handle such cases or change system in a way that those conditions do not happen.  "},{"id":2,"href":"/cs7210/lectures/lecture05/","title":"05 Consensus in Distributed Systems","parent":"Lectures","content":"Link to Lecture Slides\nPaper Trail: A Brief Tour of FLP Impossibility\nConsensus     What is Consensus?  Ability of processes in a distributed processes to agree on something like  the value of a variable current timestamp or point of computation taking an action etc.     Why is it important?  Important to make forward progress in distributed system. Reaching a consensus makes it possible for the system to be correct.   Why is it hard?  lack of global clock, network delays, faulty or malicious nodes etc.   Key Properties  Liveness/Termination: All non-faulty processes eventually decide on a value Validity/Safety: The decided value must be proposed by one of the processes Agreement/Safety: All processes must decide on the same (single) value    System Model     Asynchronous Model  Channel: Unpredictable delays and message reordering but message are not corrupted   Failure Model  At most one faulty processor Fail-stop failure model: Faulty process stops/dies. No malicious/random behaviour    Definitions     Admissible run: Run with 1 faulty processor and all messages eventually delivered (matches system model) Deciding run: Admissible run where some non-faulty processes reach a decision Totally Correct Consensus Protocol: If all admissible runs are also deciding runs Univalent configuration: Configuration of the system in which the system can reach a single value Bivalent Configuration: Configuration of the system in which it can reach more than one decisions  FLP Theorem     Paper: Impossibility of Distributed Consensus with One Faulty Process In a system with one faulty process, no consensus protocol can be totally correct. System Model: Asynchronous Model, One fail-stop faulty process Intuition of the Proof:  Consider a simple system model with just 1 faulty process which fail-stops. Is it possible to identify a starting configuration and legitimate admissible run such that system does not reach a deciding state     OR\n  Whether is it always possible to identify one admission schedule in the system with one faulty process where all messages are delivered and the system remains in bivalent configuration.  Proof in a Nutshell    "},{"id":3,"href":"/cs7210/lectures/lecture06/","title":"06 Replication","parent":"Lectures","content":"Goals of Replication     State available at more than one node =\u0026gt; Fault-tolerance, Availability Service can be provided from more than one node =\u0026gt; Scalability  Replication Modes     Active Replication (all replicas read, write and update each other) Stand-by (Primary-backup) Replication (Only one replica reads and writes, rest just follow updates from primary)  Replication Techniques       State Replication Replicated State Machine     Change in state is sent to other replicas The operation (event) is executed(applied) to every replica   + No need to execute operation again + No need to transmit large state delta   - Determining change in state can be complex and large - Operation needs to be executed and must be deterministic    Replication and Consensus    \u0026hellip;\nChain Replication     Lag in replication increases linearly with number of replicas Another option (van Renesse, Schneider, OSDI'14) is to do a synchronous replication.  "},{"id":4,"href":"/cs7210/lectures/lecture07/","title":"07 Fault Tolerance","parent":"Lectures","content":"Fault Tolerance    Some Taxonomy     Fault-Error-Failure  Fault is the problem (software bug, hardware failure) When fault is activated (because buggy part of code is executed), it causes errors Failure is the resulting behaviour   Faults: Transient / Intermittent / Permanent Failures: Fail-stop / Timing / Omission / Byzantine  Timing: System becomes \u0026ldquo;slow\u0026rdquo; Omission: some actions are missing like msg drops due to memory constraints   Managing Failures: Avoidance / Detection / Recovery / Removal  Detected failues are either removed by rollback or recovered from.    Rollback-Recovery     Rollback the system to a state before the failure Rolledback state may not be a real past state. It just needs to be consistent.  Basic Mechanisms     Checkpointing: Save the (full or incremental) state periodically Logging: Log individual write operations.  Undo Log: Log includes original values of variables. This makes it possible to undo an operation. System state can move backward. Redo Log: Log includes new values. To obtain a state, start from beginning and keep applying operations. System state can move forward.    Checkpointing Approaches    System Model\n Network is non-partitionable. Why ?? FIFO / communication channel reliability and num of failures will vary with protocol.  Uncoordinated Approach\n Processes take checkpoints independently. Problems  The failed node rollbacks to previous checkpoint. If the state of the whole system becomes inconsistent, more rollbacks may have to be performed at several nodes =\u0026gt; Domino effect. Leads to multiple useless checkpoints.    Coordinated Checkpoint\n Processes coordinate to take a consistent snapshot. Problems  How to coordinate? No synchronous clock guarantee ie no global clock. (Everybody take snapshot at 5pm) If msg delivery was reliable with bounded delay, some approach could be created.    Communication induced Checkpoint\n Use a consensus protocol: All nodes should reach a consensus that a snapshot will be taken and that they will not send any msg till it\u0026rsquo;s complete. In a way, it\u0026rsquo;s a blocking protocol. A non-blocking algo will be similar to Global Snapshot algo but it requires FIFO communication channel.  Logging\n When reconstructing state we must ensure that it\u0026rsquo;s not a inconsistent state. A crashed process might not log the last event it sent. We might apply events in wrong order. Pessimistic: First log the event then send it. Optimistic: Assume log will be persisted but make it possible to remove it\u0026rsquo;s effect if aborted Causalty-tracking: ensure causality related events are deterministically recorded  Which Method to Use?\n Workload characteristics: Frequency of updatss, size of updates, Failures characteristics: System Characteristics: cost of communication/stoage, system scale,  "},{"id":5,"href":"/","title":"CS7210 - Distributed Computing","parent":"","content":""},{"id":6,"href":"/cs7210/","title":"CS7210 - Distributed Computing","parent":"CS7210 - Distributed Computing","content":"Welcome!!\n"},{"id":7,"href":"/cs7210/labs/01_framework_abstraction/","title":"DSLabs Abstract Interface","parent":"Labs","content":" The system consists of a number of nodes which have addresses A Node can send() a message to another node. The receiving node has to handle() the message. Some nodes expose a client interface. An external user can interact with the distributed system by send()ing commandsand gettting results back. Application: Some nodes (server nodes) have contain an application object. An example of an application would be key-value in-memory store. Application is what the users of the distributed system want to access. An application takes a command as input and produces result as output. Communication is one way i.e. when a node is sending a message, it is not expecting a reply message. Request/Response semantics are implemented at client-server level.  "},{"id":8,"href":"/cs7210/labs/","title":"Labs","parent":"CS7210 - Distributed Computing","content":"    DSLabs Abstract Interface     Message Wrappers     WIP     "},{"id":9,"href":"/cs7210/lectures/","title":"Lectures","parent":"CS7210 - Distributed Computing","content":"    03 Time in Distributed Systems     04 State in Distributed Systems     05 Consensus in Distributed Systems     06 Replication     07 Fault Tolerance     "},{"id":10,"href":"/cs7210/labs/02_packets/","title":"Message Wrappers","parent":"Labs","content":"There are my abstractions:\n Message Requst and Reply MessageEnvelop Command and Result  "},{"id":11,"href":"/tags/","title":"Tags","parent":"CS7210 - Distributed Computing","content":""},{"id":12,"href":"/cs7210/labs/dslabs_client_to_server_communication/","title":"WIP","parent":"Labs","content":""}]