[{"id":0,"href":"/lectures/","title":"Lectures","parent":"CS7210 - Distributed Computing","content":"    01 - Introduction     03 - Time in D. Systems     04 - State in D. Systems     05 - Consensus     06 - Replication     07 - Fault Tolerance     "},{"id":1,"href":"/extra/","title":"Extra","parent":"CS7210 - Distributed Computing","content":"    Consistency Tradeoffs: Consistency Tradeoffs in Modern Distributed Database System Design     "},{"id":2,"href":"/labs/framework/dslabs_client_to_server_communication/","title":"WIP","parent":"DSLabs Framework","content":"Route of a Command    "},{"id":3,"href":"/lectures/lecture01/","title":"01 - Introduction","parent":"Lectures","content":"Introduction to Distributed Systems    What is a Distributed System?\nA collection of independent and autonomous processes(nodes) that interact by exchanging messages via interconnection network and appear to external users as a single coherent computing facility.\nThe phrase \u0026ldquo;single coherent computing facility\u0026rdquo; means that there is a common goal which the processes are trying to achieve together.\nSimple Model\n There nodes which communicate by exchaning messages. Communication channels are unidirectional. Channels are unreliable.  Complex Model\n Nodes have state which may change in response to events received by the node  Importance of Models in Distributed Systems\n Analyzing distributed systems based on models is more predictable. Other option would be to prototype ideas and run exhaustive tests. Testing all possible scenarios in a distributed system is quite difficult.  Components of a Model\n System Elements i.e. nodes, network Rules eg. nodes communicate by sending msg Invariats/Assumptions: eg. network delivers msgs in FIFO  How Good is a Model?\n Accurate representation of problem Tractable: easy to prototype and analyze  Challenges in Distributed System\n Asynchrony : Instant msg deliver vs bounded delay vs infinte delay Failures: Failstop vs transient vs byzantine failure Consistency: We want single up-to-date copy of data. This is complicated by concurrency, ordering, replication etc.  Fallacies of Distributed Computing The fallacies of distributed computing are a set of assertions made by L Peter Deutsch and others at Sun Microsystems describing false assumptions that programmers new to distributed applications invariably make.\n The network is reliable. Latency is zero. Bandwidth is infinite. The network is secure. Topology doesn\u0026rsquo;t change. There is one administrator. Transport cost is zero. The network is homogeneous.  Properties of a Distributed System\n Consistency: Provide correct answers High Availability Fault Tolerance  Correctness of Distributed System\n If input to distributed system is equivalent to the input to a single system performing the same function, the output of the two should be equivalent. To comment on whether the inputs are equivalent, we need to talk about the event ordering in distributed system. This is complicated by the fact that there is no global time available. Thus, different interpretations of event ordering can be made. We have to answer the question if all the nodes will even see the same order of events?  CAP Theorem\nUnder network partition, a distributed system can be either available or consistent.\n"},{"id":4,"href":"/","title":"CS7210 - Distributed Computing","parent":"","content":"External Links     Course Website Canvas Piazza Course Schedule - Fall 2021  "},{"id":5,"href":"/labs/lab02/","title":"Lab02","parent":"Labs","content":"Wait-Notify Semantics of Java     Java has a builtin wait mechanism that enable threads to become inactive while waiting for signals. The class java.lang.Object defines three methods, wait(), notify(), and notifyAll(), to facilitate this. A thread that calls wait() on any object becomes inactive until another thread calls notify() on that object. In order to call either wait() or notify the calling thread must first obtain the lock on that object. In other words, the calling thread must call wait() or notify() from inside a synchronized block.\n source: Thread Signaling by Jakob Jenkov\nSynchronized Instance Methods in Java     A synchronized instance method in Java is synchronized on the instance (object) owning the method. Thus, each instance has its synchronized methods synchronized on a different object: the owning instance. Only one thread per instance can execute inside a synchronized instance method. If more than one instance exist, then one thread at a time can execute inside a synchronized instance method per instance. One thread per instance. This is true across all synchronized instance methods for the same object (instance). Thus, in the following example, only one thread can execute inside either of of the two synchronized methods. One thread in total per instance.\n source: Java Synchronized Blocks by Jakob Jenkov\n"},{"id":6,"href":"/labs/lab03/","title":"Lab03","parent":"Labs","content":"Test 16    # Command python3 run-tests.py --lab 3 --part 2 --test-num 16 --log-level FINE --checks # Result Objects not equal to their clone. Check all classes correctly implement equals. Objects have hashCode not equal to their clone. Check all classes correctly implement hashCode. "},{"id":7,"href":"/labs/framework/01_framework_abstraction/","title":"01 DSLabs Abstract Interface","parent":"DSLabs Framework","content":"Important DSLabs Interfaces     The system consists of a number of nodes which have addresses A Node can send() a message to another node. The receiving node has to handle() the message. Some nodes expose a client interface. An external user can interact with the distributed system by send()ing commandsand gettting results back. Application: Some nodes (server nodes) have contain an application object. An example of an application would be key-value in-memory store. Application is what the users of the distributed system want to access. An application takes a command as input and produces result as output. Communication is one way i.e. when a node is sending a message, it is not expecting a reply message. Request/Response semantics are implemented at client-server level.  "},{"id":8,"href":"/labs/framework/02_workload/","title":"02 Workload","parent":"DSLabs Framework","content":"Workload    abstract class Workload { Pair\u0026lt;Command, Result\u0026gt; nextCommandAndResult() Command nextCommand() boolean hasNext() boolean hasResults() }  A workload is an ordered collection of commands and expected results. Results are optional but commands are must. The test framework initializes a Workload and maps it to a client node. The framework executes one command at a time from the Workload on the client. If results are present in the workload, after every command, the result received from the client is compared against the expected result stored in the Workload.  StandardWorkload      It\u0026rsquo;s a concrete implementation of Workload.\n  It adds two capabilities to Workload\n  Repetition: The workload ( i.e. the command in workload) can be executed specified number of times.\n  Patterns as Commands: Instead of providing exact list of command objects, it can be provided with a string patterns and it transforms them into strings. These strings are then converted to Command and Result objects by Parser. Parser is nothing but a function that takes strings as input and converts them to Command and Result Objects.\n String_Pattern \u0026quot;GET:foo_%i\u0026quot; | v +-------------------+ | Standard Workload | | doReplacements() | +-------------------+ | v +--------------+ String_Command -------\u0026gt; |Parser |-------\u0026gt; GET{ GET:foo_01 |KVStoreParser | key : \u0026quot;foo_01\u0026quot;} +--------------+     "},{"id":9,"href":"/lectures/lecture03/","title":"03 - Time in D. Systems","parent":"Lectures","content":"Time in Distributed Systems    Introduction     Distributed Systems cannot rely on Physical Clocks so they rely on Logical Clocks Reason: Physical clocks of individual processes need to be synchronized but perfect synchronization is not possible.  Why Do We Need Time     There are many scenarios in which knowing the time can be helpful. For eg. checking the correctness of the program. Same operations executed in different  Determining the order of operations executing in the system Checking correctness of the final state with the expected final state    Why Is Measuring Time Hard in DS?     Option 1: We can use the local clock on each node to record the time  Good: Since each message is timestamped, there is a global order of all the events Bad: It\u0026rsquo;s impossible to precisely synchronise all the local clocks   Option 2: Let there be a receiver/observer node. Every node in the system sends it a message when an event occurs on that node.  Due to unpredictable delays, message can arrive at the observer out of order Different Observers can report different order of events    Logical Time     Logical clocks, unlike physical clocks, progress only when an event occurs and provides a distinct timestamp to each event. Types of clocks (details left out in lecture)  Scalar Clock / Lamport Clocks Vector Clocks Matrix Clocks    Ordering Relationship Among Events     There are three type of events occur on a node: send(msg), recv(msg) and internal events. Note that when a msg is send from node n1 to node n2, two events occur  n1 registers send(msg) event n2 registers recv(msg) event.   e1 → e2 implies, e1 \u0026ldquo;happened before\u0026rdquo; e2 All events that take place on a single node are totally ordered If a node receives a msg (e1) and sends it (e2) , then e1 → e2 If a node sends a msg (e1) and it is recvd by another node(e2), then e1 → e2  Concurrent Events     If e1 and e2 are not related by \u0026ldquo;happened before\u0026rdquo; relationship, we call them concurrent. It means that we cannot conclusively state the order in which order those events occurred. Notation: e1 || e2  Logical Clocks     Logical clocks are used to assign timestamps to events. A logical clock defines a set of rules on how to advance it and how to compare two timestamps.   Monotonicity property: A logical clock must always increase Clock Consistency Condition  If e1 → e2 then, C(e1) \u0026lt; C(e2) Note that converse is not true i.e. C(e1) \u0026lt; C(e2) does not imply e1 → e2   For concurrent events, there are no guarantees. The logical timestamps can be in any order or they simply may not be comparable. Strong Clock Consistency (not mandatory for all logical clocks)  e1 → e2 ⇔ C(e1) \u0026lt; C(e2)    Lamport\u0026rsquo;s Scalar Clock     Each process had it\u0026rsquo;s own local logical clock Rules  Vector Clocks (vt)      Lamport Clock does not satisfy the Strong Clock Consistency\n  Length of vector clock is equal to number of processes in the system\n  If vt is the vector clock at a process, then vt[i] represents that process\u0026rsquo;s view of time of ith process.\n  Rules   Comparison Rules\n  Matrix Clocks     Each process maintain N x N matrix Consider process Pi,  i-th row of matrix corresponds to Pi\u0026rsquo;s own vector clock j-th row (other than i) corresponds to Pi\u0026rsquo;s view of Pj \u0026rsquo;s vector clock   Benefits  A process can know if the vector clock of every other process has progressed past a certain time, t. This can be used to delete any data which is cached for processed which are falling behind.    "},{"id":10,"href":"/labs/framework/03_state_predicates/","title":"03 State Predicates","parent":"DSLabs Framework","content":"State Predicate (Invariants)    When the framework is running tests, it needs to know whether a test passed or failed. It does so by verifying that certain conditions are met. StatePredicate are those conditions\nclass StatePredicate extends java.util.Function.Predicate\u0026lt;AbstractState\u0026gt; { String name; Function\u0026lt;AbstractState, Pair\u0026lt;Boolean, String\u0026gt;\u0026gt; predicate; public boolean test(AbstractState state); public String detail(AbstractState state); public StatePredicate negate(); public StatePredicate and(StatePredicate other); public StatePredicate or(StatePredicate other); } java.util.Function.Predicate    interface Predicate\u0026lt;T\u0026gt; { boolean test(T t); //default methods  default Predicate\u0026lt;T\u0026gt; and(Predicate\u0026lt;? super T\u0026gt; other); default Predicate\u0026lt;T\u0026gt; negate(); default Predicate\u0026lt;T\u0026gt; or(Predicate\u0026lt;? super T\u0026gt; other); }  Predicate is a standard java interface with just one abstract method signature. bool test(T) . It takes a generic T as input and produces bool as output. For any testing framework this can be a useful interface. The framework\u0026rsquo;s \u0026ldquo;conditions\u0026rdquo; should implement this interface. They take state of the system as input to test(). Then test() executes some logic and produces the result indicating whether it was a pass or a fail.  Adding Names and Messages    StatePredicate class adds two more things\n It stores a human readable name for the \u0026ldquo;condition\u0026rdquo; Along with returning whether condition was met or not, it also returns a small message indicating success or why there was a failure.   +--------------+ +-------------+ | | +----------------+ | | |AbstractState |----------\u0026gt; | StatePredicate |-------\u0026gt; |\u0026lt;bool_result, |(State of the | | predicate() | |string_msg\u0026gt; | | System) | +----------------+ | | +--------------+ +-------------+ "},{"id":11,"href":"/lectures/lecture04/","title":"04 - State in D. Systems","parent":"Lectures","content":"State in Distributed Systems    Terminology     The Global State of the system is the union of Process States and Channel States Channel State: The State of a channel between two processes is defined by the set of msgs sent from sender process but yet to be delivered to the receiver. Process State: Context stored on a client for eg variables, data structures etc. Process History: Sequence of events which occured on a process. Cut : Set of histories of all processes. The history of all processes may not be taken at the same exact time instant. Note, cut is a set of histories, it does not say anything about order of events on two different processes. Run: A totally ordered sequence of all the events belonging to a cut. Multiple runs can correspond to same cut. Consistent Cut: A cut is consistent if - \u0026ldquo;For any event e1 contained in that cut, all events that \u0026ldquo;happened before\u0026rdquo; e1 are also present in the cut.\u0026rdquo; Another way to say it, a consistent cut represents a possible global state. It respects the partial order of events. Pre-Recording Event: Events occured before the time of snapshot. Post-Recording Event: Events occurred after the snapshot is recorded.  Challenges in capturing Global State of a Distributed Systems     Instantaneous recording not possible  No Global Clock: As a result, processes cannot capture individual states at exact same time. Unreliable Network: As a result, a node cannot invoke every other node to record state instantaneously   Deterministic vs Non-Deterministic Computation  Distributed computation is non-deterministic unlike a single threaded stand alone program.    System Model     Channels are FIFO. It\u0026rsquo;s possible to build one. For eg. TCP protocol.  Finding a Consistent Cut: Algorithm in Action     Goal: Capture a consistent global state (process states + channel states) State of channel: Consider a scenario where we have a global clock. We are evaluating a system of two process : p and q. p sends msg to q at time t1. q receives msg from p at t3. We take the snapshot at time t2 such that t3 \u0026gt; t2 \u0026gt; t1. The state of channel from p to q will contain the inflight msg.  Chandy and Lamport Snapshot Algorithm      Snapshot (Global State) == State of Processes + State of Channels\n  State of process is captured when it receives the marker token for the first time.\n  Whenever a process captures it\u0026rsquo;s state, it sends out marker tokens to all other processes. So that they can capture the state of channel from the process to them.\n  State of a channel, Ch(i to j), is consists of all events received by Pj which were sent by Pi from the time Pj captured it\u0026rsquo;s local state till a marker token is received from Pi.\n  Initiator\n Save it\u0026rsquo;s local state Send marker tokens on all outgoing edges    All Other Processes: On receiving the first marker on any incoming edge\n Save state, and propagate markers on all outgoing edges Resume Execution but also save incoming messages until a marker arrives through the channel    handleMarker (channel c, Marker m) { if local_state not recorded: record local_state state(c) = EMPTY propage m else state(c) = msg received on c since local_state was recorded }  Assumptions of the algorithm  Communication channel is FIFO There are no failures and all messages arrive intact and only once   Characteristics of the algorithm  The snapshot algo doesn\u0026rsquo;t interfere with the normal execution of the processes Each process in the system records its local state and the state of its incoming channels    Features of Chandy and Lamport Algorithm:     It gives us a consistent snapshot The recorded Global State may not correspond to a state which actually existed The state, however, does represent a possible global state (a vertex in state lattice diagram from the paper)  Properties of the Global State    Benefits of Global State: Evaluate Stable Properties     What good are \u0026ldquo;recorded states\u0026rdquo; if they never really occur? They can still be useful for measuring stable properties. A property is called a stable property iff once it becomes true in a state S, it remains true for all states S’ reachable from S. Note if stable property is false, we can\u0026rsquo;t say anything about it in future states. For example:  Deadlock, If computation is complete in recorded state, then it\u0026rsquo;s also complete in actual terminal state    Definite vs Possible State     Certain unstable properties may also be important. For an unstable prop, there is no guarantee that once it becomes true it remains true forever. eg. buffer overflow, race condition. Is evaluating unstable properties on \u0026ldquo;recorded state\u0026rdquo; useful? Yes. If a undesirable unstable property was true in \u0026ldquo;recorded state\u0026rdquo;, then we know that that property can be true under some conditions in the system. This is useful. We now need to either handle such cases or change system in a way that those conditions do not happen.  "},{"id":12,"href":"/lectures/lecture05/","title":"05 - Consensus","parent":"Lectures","content":"Consensus    Link to Lecture Slides\nPaper Trail: A Brief Tour of FLP Impossibility\nConsensus     What is Consensus?  Ability of processes in a distributed processes to agree on something like the value of a variable, taking an action etc.     The problem of consensus is genuinely fundamental to distributed systems research. Getting distributed processors to agree on a value has many, many applications. For example, the problem of deciding whether to commit a transaction to a database could be decided by a consensus algorithm between a majority of replicas. You can think of consensus as ‘voting’ for a value.\n PaperTrail: A Brief Tour of FLP Impossibility\n Key Properties  Liveness/Termination: All non-faulty processes eventually decide on a value Validity/Safety: The decided value must be proposed by one of the processes Agreement/Safety: All processes must decide on the same (single) value    System Model     Asynchronous Model  There is no upper bound on the amount of time processors may take to receive, process and respond to an incoming message. Channel: Unpredictable delays and message reordering but message are not corrupted   Failure Model  At most one faulty processor Fail-stop failure model: Faulty process stops/dies. No malicious/random behaviour    Definitions     Admissible run: Run with 1 faulty processor and all messages eventually delivered (matches system model) Deciding run: Admissible run where some non-faulty processes reach a decision Totally Correct Consensus Protocol: If all admissible runs are also deciding runs Univalent configuration: Configuration of the system in which the system can reach a single value Bivalent Configuration: Configuration of the system in which it can reach more than one decisions  FLP Theorem     System Model:  At most one fail-stop faulty process    In a system with one faulty process, no consensus protocol can be totally correct. Intuition of the Proof:  Consider a simple system model with just 1 faulty process which fail-stops. Is it possible to identify a starting configuration and legitimate admissible run such that system does not reach a deciding state     OR\n  Whether is it always possible to identify one admission schedule in the system with one faulty process where all messages are delivered and the system remains in bivalent configuration.  Is Consensus Really Impossible?     Algorithms like 2PC, 3PC, Paxos, Raft etc. change some of the assumptions and system properties  Excerpts    PaperTrail: A Brief Tour of FLP Impossibility\nThe ‘FLP result’ settled a dispute that had been ongoing in distributed systems for the previous five to ten years. The problem of consensus - that is, getting a distributed network of processors to agree on a common value - was known to be solvable in a synchronous setting, where processes could proceed in simultaneous steps. In particular, the synchronous solution was resilient to faults, where processors crash and take no further part in the computation. Informally, synchronous models allow failures to be detected by waiting one entire step length for a reply from a processor, and presuming that it has crashed if no reply is received.\nThis kind of failure detection is impossible in an asynchronous setting, where there are no bounds on the amount of time a processor might take to complete its work and then respond with a message. Therefore it’s not possible to say whether a processor has crashed or is simply taking a long time to respond. The FLP result shows that in an asynchronous setting, where only one processor might crash, there is no distributed algorithm that solves the consensus problem.\n"},{"id":13,"href":"/lectures/lecture06/","title":"06 - Replication","parent":"Lectures","content":"Replication    Goals of Replication     State available at more than one node =\u0026gt; Fault-tolerance, Availability Service can be provided from more than one node =\u0026gt; Scalability  Replication Modes     Active Replication (all replicas read, write and update each other) Stand-by (Primary-backup) Replication (Only one replica reads and writes, rest just follow updates from primary)  Replication Techniques       State Replication Log Replication or Replicated State Machine     Change in state is sent to other replicas The operation (event) is executed(applied) to every replica   + No need to execute operation again + No need to transmit large state delta   - Determining change in state can be complex and large - Operation needs to be executed and must be deterministic    Replication and Consensus     When data is replicated over multiple nodes various design choices have to be made  Synchronous vs Asynchronous replication Leader based , Multi-leader or Leader-less Strong vs Weak consistency   Question: Does synchronous replication imply strong consistency and asynchronous replication imply weak consistency? Consensus based replication algorithms are used to build CP systems i.e. it provides strong consistency under network partition.  Chain Replication     Lag in replication increases linearly with number of replicas Another option (van Renesse, Schneider, OSDI'14) is to do a synchronous replication.  "},{"id":14,"href":"/lectures/lecture07/","title":"07 - Fault Tolerance","parent":"Lectures","content":"Fault Tolerance    Some Taxonomy     Fault-Error-Failure  Fault is the problem (software bug, hardware failure) When fault is activated (because buggy part of code is executed), it causes errors Failure is the resulting behaviour   Faults: Transient / Intermittent / Permanent Failures: Fail-stop / Timing / Omission / Byzantine  Timing: System becomes \u0026ldquo;slow\u0026rdquo; Omission: some actions are missing like msg drops due to memory constraints   Managing Failures: Avoidance / Detection / Recovery / Removal  Detected failues are either removed by rollback or recovered from.    Rollback-Recovery     When failure is detected, rollback the system to a consistent state before the failure Rolledback state may not be a real past state. It just needs to be consistent. Rollback can be done one operation at a time or in groups of operations called transactions.  Basic Mechanisms     Checkpointing: Save the (full or incremental) state periodically Logging: Log individual write operations.  Undo Log: Log includes original values of variables. This makes it possible to undo an operation. System state can move backward. Redo Log: Log includes new values. To obtain a state, start from beginning and keep applying operations. System state can move forward.    Checkpointing Approaches    System Model\n Network is non-partitionable. Why ?? FIFO / communication channel reliability and num of failures will vary with protocol.  Uncoordinated Approach\n Each process takes checkpoints periodically independently . Problems  The failed node rollbacks to previous checkpoint. If the state of the whole system becomes inconsistent, more rollbacks may have to be performed at several nodes =\u0026gt; Domino effect. Leads to multiple useless checkpoints. So, multiple checkpoints per process need to be stored. Checkpoints need to maintain information about send and received msgs. This will be needed to check if a set checkpoints constitute a consistent global state.    Coordinated Checkpoint\n Processes coordinate to take a consistent snapshot. Benefits  Each snapshot is consistent, no need to check at the time of recovery Only the most recent checkpoint need to be stored. Previous checkpoints can be deleted.   Problems  How to coordinate? No global clock i.e. it\u0026rsquo;s not possible to use \u0026ldquo;Everybody take snapshot at 5pm\u0026rdquo; approach If msg delivery was reliable with bounded delay, some approach could be created.    Communication induced Checkpoint\n Use a consensus protocol: All nodes should reach a consensus that a snapshot will be taken and that they will not send any msg till it\u0026rsquo;s complete. In a way, it\u0026rsquo;s a blocking protocol. A non-blocking algo will be similar to Global Snapshot algo but it requires FIFO communication channel.  Logging     When reconstructing state we must ensure that it\u0026rsquo;s not a inconsistent state. A crashed process might not log the last event it sent. We might apply events in wrong order. eg. P1 sends message to P2. P2 logs it but P1 crashes before loging the send event. Pessimistic: First log the event then send it. Optimistic: Assume log will be persisted but make it possible to remove it\u0026rsquo;s effect if aborted Causalty-tracking: ensure causality related events are deterministically recorded  Which Method to Use?\n Workload characteristics: Frequency of updates, size of updates, Failures characteristics: System Characteristics: cost of communication/stoage, system scale,  "},{"id":15,"href":"/extra/consistency_tradeoffs/","title":"Consistency Tradeoffs","parent":"Extra","content":"Consistency Tradeoffs in Modern Distributed Database System Design    By Daniel J. Abadi    The main point that paper tries to put across is this:\n Modern DDBS do not provide strong consistency guarantees under normal operations (absense of any faults) not because of CAP theorem but because of consistency-vs-Latency tradeoff. Consistency-vs-Latency tradeoff is more fundamental than CAP theorem because faults like network partitions are usually rare.\n  CAP only puts restrictions on DDBS under certain types of failures. During normal functioning, DDBS can work without constraints. In other words, until a partition occurs, the database can be both consistent and available. If a DDBS provides lower consistency under normal operation, it does so due to the tradeoff between Latency and Consistency instead of CAP theorem In CAP\u0026rsquo;s proof, authors use \u0026ldquo;Linearizability\u0026rdquo; form of consistency  Data Replication    It can be done by three means\n Leaderless Replication Single Leader Replication Multi-Leader Replication or Master-Master Replication  In all forms of replication, there is tension between consistency and latency.\n If synchronous replication is used between nodes, consistency improves but cost is paid in latency Asynchronous replication is faster but replicas can diverge if different nodes apply changes in different orders In asynchronous replication, if nodes try to decide on a global order of events, latency increases.  Example: Amazon\u0026rsquo;s Dynamo\n A single key is stored at multiple nodes. One of these nodes becomes master for an operation and drives it. =\u0026gt; Multi-Leader The leader propagates the operation to other owners of the key. First W writers are updated synchronously while the rest are updated asynchronously.  PACELC    if (Partition) Availability vs Consistency Else Latency vs Consistency Example: MongoDB : PA/EC (Under Partition, it\u0026rsquo;s Available ELSE it\u0026rsquo;s Consistent)\n Under normal operation, single master and linearisable system Under partition, all nodes accept writes and thus system stays available but in consistent.  "},{"id":16,"href":"/labs/framework/","title":"DSLabs Framework","parent":"Labs","content":"    01 DSLabs Abstract Interface     02 Workload     03 State Predicates     WIP     "},{"id":17,"href":"/exams/","title":"Exams","parent":"CS7210 - Distributed Computing","content":"    "},{"id":18,"href":"/readings/w02_logical_time/","title":"L03 - Logical Time","parent":"Readings","content":"Logical Time: A Way to Capture Causality in Distributed Systems    - M. Raynal, M. Singha\nA Way to Capture Causality in Distributed Systems    Link to Paper\nThis paper is an easy read and not very long. Moreover, first few pages just repeat the same thing over and over. So, you can read through it in a couple of hours.\nBig Ideas\n Global/physical clocks don’t work for distributed processes. So, we use a system of logical clocks where each process has its own clock. There is no way to determine a total order on the events, we work with partial ordering.  Abstract\n Logical time is better suited for tracking causality in distributed systems than physical time Paper reviews three ways to define logical time (scalar, vector, matrix) in distributed systems  1 Introduction\n Model: A group of distributed processes communicating by passing messages instead of sharing memory. Messages have finite but unpredictable delays Three types of events can occur at a process: internal events, msg send, and msg recv. “msg send” and “msg recv” events establish causality between the two processes ⇒ partial ordering Knowing causality helps to solve a number of problems which are listed in paper. I found the following one interesting  Concurrency measure: “\u0026hellip;All events that are not causally related can be executed in parallel. Thus, an analysis of the causality in a computation gives an idea of the concurrency in the program”   Physical( or wall) clocks don’t work for distributed systems because they are loosely synchronized. So, we turn to logical clocks. What is a logical clock? Paper presents  a general framework of a system of logical clocks three popular systems of logical clocks - scalar, vector and matrix efficient implementations of logical clocks    A Model of Distributed Execution    2.1 General Context    A distributed program consists of n processes which communicate by passing messages. Communication delay is finite but unpredictable. There is no global clock. Communication is asynchronous i.e. processes do not wait for msg delivery.\n2.2 Distributed Execution     All events occurring on a single process have a total order. P1 sends msg M (event e1s) and P2 receives this msg (event e2r), then ⇒ All events the occurred on P1 upto e1s “happened before” any event the occurred on P2 beyond e2r Two events can either be related by “happened before” relationship else they are concurrent ie we cant decide their order Notation  A → B A happened before B A || B A and B are concurrent    2.3 Distributed Executions at an Observation Level    Consider a set of events, E and a bunch of “happened before” relationships among them. An observation level is a subset E’ of E and a subset of all “happened before” relationships which relate events within E’.\n"},{"id":19,"href":"/readings/w03_consistent_global_states/","title":"L04 - Consistent Global States","parent":"Readings","content":"Consistent Global States of Distributed Systems Fundamental Concepts and Mechanisms    Link to Paper\n1 Introduction     What is global state of a distributed system?\nIt is the union of the states of the individual processes.\nWhy do we study construction of global state?\nA large class of problems in distributed computing can be cast as executing some action when the state of the system satisfies a particular condition. Thus, the ability to construct a global state and evaluate a predicate over such a state constitutes the core of solutions to many problems in distributed computing\nWhy is it a hard problem?\nuncertainties due to message delays and relative computation speeds prevent a process from drawing conclusions about the instantaneous global state of the system to which it belongs.\n.separate process [can] construct [consistent] global states but still reach conflicting conclusions regarding some global property. This \u0026ldquo;relativistic effect\u0026rdquo; is inherent to all distributed computations and limits the class of system properties that can be effectively detected.\nContents of Paper\n 2 Asynchronous Model of Distributed System:     Collection of sequential processes with unbounded relative speeds of computation Network  every process can communicate with rest of the processes message delays are unbounded and unpredictable ⇒ out-of-order message    3 Distributed Computations     The activity of each sequential process is modeled as executing a sequence of events  Event can be either (1) internal event, (2) send(m) event or (3) receive(m) event   The local history of process Pi during the computation is a (possibly infinite) sequence of events hik = ei1 ei2\u0026hellip;eik.  Note the local history has a total order   The global history of the computation is a set H = h1 ∪ … ∪ hn containing all of it\u0026rsquo;s events.  Note the global history does not have total order   Formally, a distributed computation is a partially ordered set (poset) defined by the pair (H, →). Space-time Diagram  Arrows represent message passing Horizontal axis is time Note there are three kinds of events: internal, send(m), receive(m) Some events are related by happened before (→)relationship, others are concurrent (||)       4 Global States, Cuts and Runs      5 Monitoring Distributed Computation     First strategy towards constructing global state Active Monitor: A single process (called monitor/observer) periodically polls all the process of distributed system for their local states. Once it receives response from all processes, it constructs a global state. Problem with this approach:  Suppose some process P1 sends a message to another process P2. The local states obtained by the monitor process may be such that send(msg) on P1 is not captured by the monitor but recv(msg) on P2 is captured. But a send event always happens before the recv event. A global state constructed using this strategy may not be possible in reality under any circumstances. Thus, any property/predicate calculated from this global state will be incorrect.   Example showing effect of this problem: Detecting deadlock in a distributed system consisting of single threaded process communicating with each other.  We construct a dependency graph of processes waiting on each other. If a cycle exists in graph, then we have a deadlock Due to the problem discussed above, we may see a deadlock when it does not exist and vice versa    6 Consistency     Causal Precedence:   A cut C is consistent if for any event e in C, C also contains any event e' which \u0026ldquo;happened before\u0026rdquo; e\n  In Space-Time Diagram: if all arrows that intersect the cut have their bases to the left and heads to the right of it, then the cut is consistent; otherwise it is inconsistent\n  Just as a scalar time value denotes a particular instant during a sequential computation, the frontier of a consistent cut establishes an “instant” during a distributed computation.\n  A run R is said to be consistent if for all events, e → e' implies that e appears before e' in R\n    7 Observing Distributed Computation     In this section, we look at the second strategy to build distribute state: The Passive Observer All processes in the system send updates about every local event to the passive observer. The passive observer constructs a global state from these events. This global state may be consistent, inconsistent or outright invalid since updates can arrive at the passive observer in any order. eg. recv(msg) may arrive before send(msg), two internal events on the same process may also arrive out of order. To ensure that this strategy produces only consistent global state, we need to ensure two conditions   Condition 1: order of the events is correct i.e. if e → e', then e should appear before e' in observer\u0026rsquo;s history\nCondition 2: completeness of events i.e. if e → e' and e' is observed then e must be observed as well.\n 8 Logical Clocks     Continuing the discussion of previous section If every process maintains scalar logical clock and stamps updates to the observer with logical-event-timestamp, it is possible to meet condition 1 by sorting events based on this logical timestamp at the Observer. To meet condition 2, we need to impose a constraint on channel between pairs of processes. The channel needs to be FIFO. This ensures that updates from a process arrive in order and are not dropped. If we can say that latest event from each process has timestamp greater than T, then events up to T are complete on the Observer.  9 Causal Delivery     This extends the concept of FIFO channels between pairs of processes Events sent from all the processes to the observer are delivered to the observer in an order which respects partial ordering of events i.e. events of a single process are totally ordered + send() and corresponding recv() events are correct ordered. Delivering events to the observer while respecting above property is called Causal Delivery  10 Constructing the Causal Precedence Relation     Definition: Strong Clock Condition e → e' ⇔ T(e) \u0026lt; T(e') Vector Clocks satisfy Strong Clock Condition  11 Implementing Causal Delivery with Vector Clocks    "},{"id":20,"href":"/labs/","title":"Labs","parent":"CS7210 - Distributed Computing","content":"    Lab02     Lab03     DSLabs Framework     01 DSLabs Abstract Interface     02 Workload     03 State Predicates     WIP       Some useful links about virtual box\n What is VirtualBox closed source and VirtualBox OSE? https://wiki.debian.org/VirtualBox https://blogs.oracle.com/virtualization/friday-spotlight:-virtualbox-extension-pack-guest-additions#  "},{"id":21,"href":"/readings/","title":"Readings","parent":"CS7210 - Distributed Computing","content":"    L03 - Logical Time: A Way to Capture Causality in Distributed Systems     L04 - Consistent Global States: Fundamental Concepts and Mechanisms     W07 - Amazon Aurora: On Avoiding Distributed Consensus for I/Os, Commits, and Membership Changes     W07 - Spanner: Google’s Globally-Distributed Database     "},{"id":22,"href":"/tags/","title":"Tags","parent":"CS7210 - Distributed Computing","content":""},{"id":23,"href":"/readings/07_amazon_aurora/","title":"W07 - Amazon Aurora","parent":"Readings","content":"Amazon Aurora: On Avoiding Distributed Consensus for I/Os, Commits, and Membership Changes    - Alexandre Verbitski, Anurag Gupta, Debanjan Saha, James Corey, Kamal Gupta Murali Brahmadesam, Raman Mittal, Sailesh Krishnamurthy, Sandor Maurice Tengiz Kharatishvilli, Xiaofeng Bao\nLinks      The Morning Paper\n  Just How \u0026ldquo;Global\u0026rdquo; is Amazon Aurora? - CockroachLabs\n  Notes on Amazon Aurora\n  diegopacheco/notes\n  https://www.binwang.me/2020-09-16-Aurora-Database.html\n  "},{"id":24,"href":"/readings/w07spanner/","title":"W07 - Spanner","parent":"Readings","content":"Links     Spanner and Open Source Implementations OSDI12 - Spanner: Google’s Globally-Distributed Database - Youtube  Spanner FAQ    Source: MIT 6.824 - Distributed Systems \u0026gt; Spanner FAQs\nQ: What is time? A: 13:00:00.000 April 7th 2020 UTC is a time. Time advances at a steady rate of one second per second. Q: What is a clock? A: A clock is an oscillator driving a counter. The oscillator should tick at a steady known rate (e.g. one tick per second). The counter must initially be synchronized to a source of the correct time; after that, the oscillator's ticks cause the counter to advance the time. If the oscillator ticks at precisely the right rate, the counter will advance in precise synchrony with the correct time. In real life, the oscillator ticks at a varying and somewhat incorrect frequency, so the counter gradually drifts away from the correct time. Q: What time is it? A: In order to provide a notion of univeral time (UTC), a bunch of government laboratories individually maintain highly accurate clocks, and compare with each other to produce a consensus on what time it is. The current time is broadcast in a variety of ways, such as WWV, GPS, and NTP, so that ordinary people can know the official time. Those broadcasts take a varying and hard-to-predict amount of time to reach listeners, so you never know the exact time. Q: What is an atomic clock? A: A highly stable oscillator. There are two main technologies that go by the name \u0026quot;atomic clock\u0026quot;: rubidium clocks and cesium clocks. Both exploit changes in the state of the outer electron, which involve specific quanta of energy and thus wavelength. One can tune a signal generator to precisely that wavelength by watching how excited the electrons are. An atomic clock is just the oscillator part of a clock; on startup, it must be synchronized somehow to UTC, typically by radio broadcasts such as GPS. Q: What kind of atomic clock does Spanner use? A: Sadly the paper doesn't say. Rubidium clocks are typically a few thousand dollars (e.g. https://thinksrs.com/products/fs725.html). Rubidium clocks drift by perhaps a few microseconds per week, so they need to be re-synchronized to UTC (typically by GPS) every once in a while. Cesium clocks cost perhaps $50,000; the HP 5071A is a good example. A cesium clock doesn't drift. Of course, any one clock might fail or suffer a power failure, so even with perfect cesium clocks you still need more than one and the ability to synchronize to UTC. My guess, based on price, is that Spanner uses rubidium clocks that are synchronized with GPS receivers. Q: How does external consistency relate to linearizability and serializability? A: External consistency seems to be equivalent to linearizability, but applied to entire transactions rather than individual reads and writes. External consistency also seems equivalent to strict serializability, which is serializability with the added constraint that the equivalent serial order must obey real time order. The critical property is that if transaction T1 completes, and then (afterwards in real time) transaction T2 starts, T2 must see T1's writes. Q: Why is external consistency desirable? A: Suppose Hatshepsut changes the password on an account shared by her workgroup, via a web server in a datacenter in San Jose. She whispers the new password over the cubicle wall to her colleage Cassandra. Cassandra logs into the account via a web server in a different datacenter, in San Mateo. External consistency guarantees that Cassandra will observe the change to the password, and not, for example, see a stale replica. Q: Could Spanner use Raft rather than Paxos? A: Yes. At the level of this paper there is no difference. At the time Spanner was being built, Raft didn't exist, and Google already had a tuned and reliable Paxos implementation. Have a look at the paper Paxos Made Live by Chandra et al. Q: What is the purpose of Spanner's commit wait? A: Commit wait ensures that a read/write transaction does not complete until the time in its timestamp is guaranteed to have passed. That means that a read/only transaction that starts after the read/write transaction completes is guaranteed to have a higher timestamp, and thus to see the read/write transaction's writes. This helps fulfil the guarantee of external consistency: if T1 completes before T2 starts, T2 will come after T1 in the equivalent serial order (i.e. T2 will see T1's writes). Q: Does anyone use Spanner? A: It's said that hundreds of Google services depend on Spanner. The paper talks about its use by Google's advertising system. Google's Zanzibar Authorization system uses Spanner. It's offered as a service to Google's cloud customers in the form of Cloud Spanner. The CockroachDB open-source database is based on the Spanner design. Paper Question Source: MIT 6.824\nSuppose a Spanner server\u0026rsquo;s TT.now() returns correct information, but the uncertainty is large. For example, suppose the absolute time is 10:15:30, and TT.now() returns the interval [10:15:20,10:15:40]. That interval is correct in that it contains the absolute time, but the error bound is 10 seconds. See Section 3 for an explanation TT.now(). What bad effect will a large error bound have on Spanner\u0026rsquo;s operation? Give a specific example.\n"}]